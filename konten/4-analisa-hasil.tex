% Ubah judul dan label berikut sesuai dengan yang diinginkan.
\section{Results}
\label{sec:analisahasil}

After the text has done, the results obtained from the LLM model will be analyzed. The results obtained will be analyzed based on how much resources it uses. Results also evaluated by the relevance and accuracy of the answers given. 

\subsection{RASA Framework}
The first is testing using the \emph{RASA} framework. From this testing, it was found that RASA has advantages in terms of easy \emph{deployment}. However, RASA has shortcomings in terms of \emph{training}. RASA conducts \emph{training} using existing data. RASA uses YAML files to store its configurations. These configurations must contain the data to be used. Conversations will be limited to this existing data. This causes RASA to be unable to respond to questions outside this data. If users provide input different from the data, RASA will struggle to understand the user's intent. This results in RASA being unable to provide answers that match the user's questions. RASA can be used effectively if the user has an extensive question and answer dataset so that RASA can provide appropriate answers. This constraint significantly hinders RASA’s ability to handle queries or interactions that deviate from the predefined datasets. When faced with unfamiliar user inputs, RASA often struggles to accurately interpret the user's intent, leading to responses that do not align with the user's actual questions or needs. While RASA can be effective when a comprehensive dataset of questions and answers is available, this necessity limits its flexibility and scalability.

RASA also lacks an active community, which means the development of RASA is slower compared to other \emph{frameworks}. RASA’s development suffers from a lack of robust community support. Unlike platforms such as \emph{HuggingFace}, which thrive on active community contributions and continuous advancements, RASA's community is relatively inactive. This lack of engagement results in slower updates and enhancements, contributing to RASA being perceived as outdated compared to more dynamic and community-driven frameworks. The culmination of these factors makes RASA a less optimal choice for deployment in projects that demand adaptability and ongoing development support. This causes RASA to be less optimal for use in this project.

The creation of a \emph{chatbot} using the Llama.cpp \emph{framework} with the aya-23-8B-Q4-K-M model requires 3.3 GB of RAM and 5.1 GB of VRAM, and takes 6 seconds to process a response. The resulting \emph{chatbot} has a \emph{Faithfulness} score of 93.095\%, an \emph{Answer Relevancy} score of 98.299\%, and an \emph{Answer Correctness} score of 77.093\%. Next, testing was conducted using RAGAs to evaluate the overall performance of RAG. This involved testing the aya-23-8B-Q4-K-M model used in the final results. The test consisted of 75 questions created and answered by humans. These questions were then also answered by the RAG model, and the answers were compared to the human answers. The metrics assessed in this testing included Faithfulness, Answer Relevancy, and Answer Correctness. This evaluation allowed for a detailed comparison between the human-generated answers and those provided by the \emph{chatbot}, highlighting the model's strengths and areas for improvement. 
\subsection{Langchain and HuggingFace Framework }
The next is testing using a combination of LangChain and HuggingFace. From the test results obtained in Table  \ref{tab:hasilrag}
\begin{table}[!htbp]
  \caption{Langchain and HuggingFace results}
  \label{tab:hasilrag}
  \centering
  \begin{tabular}{llll}
    \toprule
    Name                      & R/V(GB) & T(s)  & Result \\
    \midrule
    bigscience/bloom-7b1      & 2.5/8.2    &27   & Accurate, but \\ \cite{muennighoff2022crosslingual}                          &          &           & not natural \\  
    \\
    bigscience/bloom-1b7
    \cite{muennighoff2022crosslingual}      & 3.1/6.9   &15    & Accurate, but \\
                              &          &           & not natural \\ 
    \\ 
    sail/Sailor-4B \cite{dou2024sailor}           & 4.3/6.3    &20   & Inaccurate \\
    \\
    sail/Sailor-0.5B \cite{dou2024sailor}          & 2.3/2.7   &12    & Inaccurate \\
    \\
    indonlp/cendol-mt5-       & 3.0/0.7  &22     & Accurate, but \\
    small-inst \cite{indonlp1,indonlp2, indonlp3, indonlp4, indonlp5, indonlp6, indonlp7}              &          &           & not natural \\ 
    \\
    indonlp/cendol-llama2-    & 2.5/0.7   &25      & Inaccurate \\
    7b-chat \cite{indonlp1,indonlp2, indonlp3, indonlp4, indonlp5, indonlp6, indonlp7}                  &          &           &  \\
    \\
    Yellow-AI-NLP/komodo-     & 5.0/7.3   &30    & Inaccurate \\
    7b-base \cite{owen2024komodo}                  &          &           &  \\
    \\
    cahya/gpt2-small-         & 2.4/0.4   &11    & Inaccurate \\
    indonesian-522M \cite{cahya_llm}          &          &           &  \\
    \\
    kalisai/Nusantara-7b-     & 3.2/9.9   &19    & Accurate, but \\
    Indo-Chat \cite{zulfikar_aji_kusworo_2024}        &          &           & not natural \\
    \\
    kalisai/Nusantara-4b-     & 4.4/6.3   &12    & Accurate, but \\
    Indo-Chat  \cite{zulfikar_aji_kusworo_2024}               &          &           & not natural \\
    \bottomrule
  \end{tabular}
\end{table}


\subsection{Framework Langchain dan Llama.cpp}
Next is testing using a combination of LangChain and Llama.cpp. From the test results obtained in Table \ref{tab:hasilllama}
\begin{table}[!htbp]
  \caption{llama.cpp Results}
  \label{tab:hasilllama}
  \centering
  \begin{tabular}{llll}
    \toprule
    Name                      & R/V(GB) & T(s)  & Result \\
    \midrule
    bigscience/bloom-7b1      & 2.5/8.2    &18   & Accurate, but \\ \cite{muennighoff2022crosslingual}                          &          &           & not natural \\  
    \\
    bigscience/bloom-1b7
    \cite{muennighoff2022crosslingual}      & 3.1/6.9   &15    & Accurate, but \\
                              &          &           & not natural \\ 
    \\ 
    Merak-7B-v4-model-Q4 \cite{Merak}       & 3.4/5.3    &13   & Accurate, but \\
                              &          &           & not natural \\
    \\
    Merak-7B-v4-model-Q5 \cite{Merak}       & 3.5/6.0   &15    & Accurate, but \\
                              &          &           & not natural \\
    \\
    aya-23-8B-Q6-K \cite{aryabumi2024aya}   & 3.3/7.0  &10     & Accurate \\
    \\
    aya-23-8B-Q4-K-M \cite{aryabumi2024aya} & 3.3/5.1   &6      & Accurate, but \\
                              &          &           & not optimal \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Overall Testing with RAGAs}

To observe overall performance of RAG process. This experiment utilized the RAGAs framework, specifically testing the model aya-23-8B-Q4-K-M. We tested the model by comparing its responses to 75 human-answered questions across metrics such as Faithfulness, Answer Relevancy, and Answer Correctness.
To test the RAG pipeline is challenging because the RAG pipeline consists of several different components to be tested. When evaluating the RAG pipeline, both components must be evaluated separately but simultaneously to understand if and where the RAG pipeline still needs improvement. RAGAs is a framework that allows easy evaluation of RAG \cite{es2023ragas}. RAGAs evaluate the results of RAG by creating a dataset, which is then supplemented with answers from the RAG model. This dataset consists of four parts:

\begin{enumerate}[nolistsep]
\item \emph{Question}: A list of questions to be tested on the model. These questions can be manually created by humans or generated using synthetic features available in RAGAs.
\item \emph{Ground truths}: The correct answers to the questions. This section can also be created by humans or generated synthetically. This information is essential for the \emph{context recall} metric.
\item \emph{Answer}: The responses produced by the RAG pipeline. This is the output from the tested RAG model.
\item \emph{Contexts}: The context retrieved from external knowledge sources used to answer the questions.
\end{enumerate}

Using this dataset, RAGAs then evaluates the results of the RAG model.
The first test used a bi-encoder for the retriever. The results showed a Faithfulness score of 45.415\%, an Answer Relevancy score of 88.965\%, and an Answer Correctness score of 54.193\%. These scores indicated that the generated answers were lacking in Faithfulness and Answer Correctness, meaning the answers were far from factual. This was attributed to the context not matching the given questions, causing the model to respond without using the entire or correct context, leading to low scores in Faithfulness and Answer Correctness.

The second test used a cross-encoder for the retriever, as cross-encoders provide better results with a small data source, though they take longer to process information compared to bi-encoders. This test yielded a Faithfulness score of 79.931\%, an Answer Relevancy score of 96.236\%, and an Answer Correctness score of 62.596\%. These scores showed improved answers in all aspects. However, the Faithfulness and Answer Correctness scores still did not reach the desired levels. This was due to the context being truncated by the system's method of dividing data into smaller pieces with TextSplitter.

The third test used the same configuration as the second but divided the data differently, grouping sections by context. This test resulted in a Faithfulness score of 86.269\%, an Answer Relevancy score of 96.536\%, and an Answer Correctness score of 65.196\%. These scores showed further improvements in all aspects, though the Faithfulness and Answer Correctness scores still fell short of the desired levels. This was because the generated answers were longer than necessary, influenced by the model's high temperature setting, which caused it to provide more detailed explanations. Additionally, the model was prompted to explain its answers, resulting in longer responses.

The fourth test also used the same configuration as the third but with a lower temperature setting and a prompt for concise yet detailed answers. This test produced a Faithfulness score of 93.095\%, an Answer Relevancy score of 98.299\%, and an Answer Correctness score of 77.093\%. These results indicated significant improvements in all aspects, though the Answer Correctness score still did not reach 90\%. However, considering that these scores were compared to human answers, the results were deemed satisfactory.

A comparison table of all the test results can be seen in Table \ref{tab:hasilragas}, which displays the Faithfulness, Answer Relevancy, and Answer Correctness scores for each test conducted.

Results from all tests are compiled in Table \ref{tab:hasilragas}, with Faithfulness (F), Answer Relevancy (AR), Answer Correctness (AC).


\begin{table}[!htbp]
  \caption{RAGAs Results}
  \label{tab:hasilragas}
  \centering
  \begin{tabular}{llll}
    \toprule
    No & F (\%) & AR (\%) & AC {\%} \\
    \midrule
    1& 45,415  & 88,965 & 54,193 \\
    2& 79,931 & 96,236 & 62,596 \\
    3& 86,269 & 96,536 & 65,196 \\
    4& 93,095 & 98,299 & 77,093 \\
    \bottomrule
  \end{tabular}
\end{table}
% Ubah paragraf-paragraf pada bagiaOveran ini sesuai dengan yang diinginkan.

% % Contoh input beberapa gambar pada halaman.
% \begin{figure*}
%   \centering
%   \subfloat[Hasil A]{\includegraphics[width=.4\textwidth]{example-image-a}
%     \label{fig:hasila}}
%   \hfil
%   \subfloat[Hasil B]{\includegraphics[width=.4\textwidth]{example-image-b}
%     \label{fig:hasilb}}
%   \caption{Contoh input beberapa gambar.}
%   \label{fig:hasil}
% \end{figure*}

% \lipsum[16-18]

% % Contoh input potongan kode dari file.
% \lstinputlisting[
%   language=Python,
%   caption={Program perhitungan bilangan prima.},
%   label={lst:bilanganprima}
% ]{program/bilangan-prima.py}

% \lipsum[19-20]
